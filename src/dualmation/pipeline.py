"""
End-to-end DualAnimate pipeline orchestrator.

Connects all modules: embeddings â†’ LLM code gen â†’ diffusion visual gen â†’
compositor â†’ reward scoring. Provides the full inference and training loop.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from pathlib import Path

import torch
from PIL import Image

from dualmation.compositor.compositor import AlphaCompositor, CompositeConfig
from dualmation.reward.reward_model import RewardConfig, RewardModel, RewardScore

logger = logging.getLogger(__name__)


@dataclass
class PipelineConfig:
    """Configuration for the full DualAnimate pipeline.

    Attributes:
        concept: The educational concept to animate.
        llm_model: HuggingFace model ID for Manim code generation.
        diffusion_model: HuggingFace model ID for visual background generation.
        embedding_dim: Dimension of the shared multimodal embedding space.
        output_dir: Directory to save generated outputs.
        device: Compute device (auto-detected if None).
        use_embeddings: Whether to use the multimodal embedding module.
        composite_config: Alpha compositor configuration.
        reward_config: Reward model configuration.
    """

    concept: str = "Explain gradient descent visually"
    llm_model: str = "codellama/CodeLlama-7b-hf"
    diffusion_model: str = "stabilityai/stable-diffusion-2-1"
    embedding_dim: int = 512
    output_dir: str = "outputs"
    device: str | None = None
    use_embeddings: bool = True
    composite_config: CompositeConfig = field(default_factory=CompositeConfig)
    reward_config: RewardConfig = field(default_factory=RewardConfig)


@dataclass
class PipelineResult:
    """Result from a single pipeline run.

    Attributes:
        concept: The input concept.
        generated_code: The Manim Python code generated by the LLM.
        background_images: Diffusion-generated background images.
        composited_frames: Final composited output frames.
        reward: Reward score from the RL scorer.
        concept_embedding: The concept's embedding in the shared space.
    """

    concept: str
    generated_code: str = ""
    background_images: list[Image.Image] = field(default_factory=list)
    composited_frames: list[Image.Image] = field(default_factory=list)
    reward: RewardScore | None = None
    concept_embedding: torch.Tensor | None = None


class DualAnimatePipeline:
    """Full DualAnimate inference pipeline.

    Orchestrates the flow:
    1. Encode concept â†’ multimodal embedding
    2. LLM generates Manim code (Brain 1: Logic)
    3. Diffusion generates visual background (Brain 2: Aesthetics)
    4. Alpha compositor merges foreground + background
    5. Reward model scores the output
    6. (Training mode) RL feedback updates LLM and diffusion

    Args:
        config: Pipeline configuration.
    """

    def __init__(self, config: PipelineConfig) -> None:
        self.config = config
        self.device = config.device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Modules are lazy-loaded to avoid loading everything at init
        self._code_encoder = None
        self._visual_encoder = None
        self._code_generator = None
        self._visual_generator = None
        self._compositor = AlphaCompositor(config.composite_config)
        self._reward_model = RewardModel(config.reward_config, device=self.device)

    @property
    def code_encoder(self):
        """Lazy-load the code encoder."""
        if self._code_encoder is None:
            from dualmation.embeddings.code_encoder import CodeEncoder

            self._code_encoder = CodeEncoder(
                embedding_dim=self.config.embedding_dim
            ).to(self.device)
        return self._code_encoder

    @property
    def visual_encoder(self):
        """Lazy-load the visual encoder."""
        if self._visual_encoder is None:
            from dualmation.embeddings.visual_encoder import VisualEncoder

            self._visual_encoder = VisualEncoder(
                embedding_dim=self.config.embedding_dim
            ).to(self.device)
        return self._visual_encoder

    @property
    def code_generator(self):
        """Lazy-load the LLM code generator."""
        if self._code_generator is None:
            from dualmation.llm.code_generator import ManimCodeGenerator

            self._code_generator = ManimCodeGenerator(
                model_name=self.config.llm_model,
                device=self.device,
            )
        return self._code_generator

    @property
    def visual_generator(self):
        """Lazy-load the diffusion visual generator."""
        if self._visual_generator is None:
            from dualmation.diffusion.visual_generator import VisualGenerator

            self._visual_generator = VisualGenerator(
                model_name=self.config.diffusion_model,
                device=self.device,
            )
        return self._visual_generator

    def run(self, concept: str | None = None) -> PipelineResult:
        """Execute the full pipeline for a concept.

        Args:
            concept: Override concept from config if provided.

        Returns:
            PipelineResult with all generated artifacts and scores.
        """
        concept = concept or self.config.concept
        result = PipelineResult(concept=concept)

        logger.info("ðŸš€ Pipeline starting for concept: %s", concept)

        # Step 1: Generate concept embedding (optional)
        embedding = None
        if self.config.use_embeddings:
            try:
                embedding = self.code_encoder.encode(concept)
                result.concept_embedding = embedding
                logger.info("âœ… Concept embedding generated: shape=%s", embedding.shape)
            except Exception as e:
                logger.warning("âš ï¸ Embedding generation failed, continuing without: %s", e)

        # Step 2: LLM â†’ Manim code (Brain 1: Logic)
        try:
            code = self.code_generator.generate_with_embedding(
                concept=concept, embedding=embedding
            )
            result.generated_code = code
            logger.info("âœ… Manim code generated: %d chars", len(code))
        except Exception as e:
            logger.error("âŒ Code generation failed: %s", e)
            result.generated_code = f"# Code generation failed: {e}"

        # Step 3: Diffusion â†’ visual background (Brain 2: Aesthetics)
        try:
            backgrounds = self.visual_generator.generate_with_embedding(
                concept=concept, embedding=embedding
            )
            result.background_images = backgrounds
            logger.info("âœ… Background generated: %d images", len(backgrounds))
        except Exception as e:
            logger.warning("âš ï¸ Background generation failed: %s", e)

        # Step 4: Compositor (if we have both layers)
        # Note: In full production, Manim would render foreground frames first.
        # Here we demonstrate the compositor with the diffusion output.
        if result.background_images:
            logger.info("âœ… Compositor ready (awaiting Manim render for foreground)")

        # Step 5: Reward scoring
        try:
            visual = result.background_images[0] if result.background_images else None
            reward = self._reward_model.score(
                code=result.generated_code,
                visual=visual,
                concept=concept,
                concept_embedding=embedding,
            )
            result.reward = reward
            logger.info(
                "âœ… Reward: total=%.3f (align=%.3f, visual=%.3f, compile=%.3f)",
                reward.total,
                reward.concept_alignment,
                reward.visual_quality,
                reward.compilation_success,
            )
        except Exception as e:
            logger.warning("âš ï¸ Reward scoring failed: %s", e)

        # Save outputs
        self._save_outputs(result)

        logger.info("ðŸŽ¬ Pipeline complete for: %s", concept)
        return result

    def _save_outputs(self, result: PipelineResult) -> None:
        """Save all generated artifacts to the output directory."""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save generated code
        code_path = output_dir / "generated_scene.py"
        code_path.write_text(result.generated_code)
        logger.info("Saved code: %s", code_path)

        # Save background images
        for i, img in enumerate(result.background_images):
            img_path = output_dir / f"background_{i:04d}.png"
            img.save(img_path)
            logger.info("Saved background: %s", img_path)

        # Save composited frames
        for i, frame in enumerate(result.composited_frames):
            frame_path = output_dir / f"composited_{i:04d}.png"
            frame.convert("RGB").save(frame_path)

        # Save reward summary
        if result.reward:
            reward_path = output_dir / "reward_summary.txt"
            reward_path.write_text(
                f"Concept: {result.concept}\n"
                f"Total Reward: {result.reward.total:.4f}\n"
                f"Concept Alignment: {result.reward.concept_alignment:.4f}\n"
                f"Visual Quality: {result.reward.visual_quality:.4f}\n"
                f"Compilation Success: {result.reward.compilation_success:.4f}\n"
                f"Compilation Output: {result.reward.compilation_output}\n"
            )


def main():
    """CLI entry point for running the DualAnimate pipeline."""
    import argparse

    logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(name)s | %(message)s")

    parser = argparse.ArgumentParser(description="DualAnimate Pipeline")
    parser.add_argument(
        "--concept",
        type=str,
        default="Explain gradient descent visually",
        help="Concept to animate",
    )
    parser.add_argument("--output-dir", type=str, default="outputs")
    parser.add_argument("--llm-model", type=str, default=PipelineConfig.llm_model)
    parser.add_argument("--diffusion-model", type=str, default=PipelineConfig.diffusion_model)
    parser.add_argument("--no-embeddings", action="store_true")

    args = parser.parse_args()

    config = PipelineConfig(
        concept=args.concept,
        llm_model=args.llm_model,
        diffusion_model=args.diffusion_model,
        output_dir=args.output_dir,
        use_embeddings=not args.no_embeddings,
    )

    pipeline = DualAnimatePipeline(config)
    result = pipeline.run()

    print(f"\n{'='*60}")
    print(f"DualAnimate Result for: {result.concept}")
    print(f"{'='*60}")
    print(f"Code length: {len(result.generated_code)} chars")
    print(f"Backgrounds: {len(result.background_images)} images")
    if result.reward:
        print(f"Total reward: {result.reward.total:.4f}")
    print(f"Outputs saved to: {config.output_dir}")


if __name__ == "__main__":
    main()
