"""
End-to-end DualAnimate pipeline orchestrator.

Connects all modules: embeddings â†’ LLM code gen â†’ diffusion visual gen â†’
compositor â†’ reward scoring. Uses ExperimentConfig from configs/ for
reproducible, configurable experiments.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from pathlib import Path

import torch
from PIL import Image

from dualmation.compositor.compositor import AlphaCompositor, CompositeConfig
from dualmation.experiment.config import ExperimentConfig, load_config
from dualmation.experiment.reproducibility import set_seed, save_environment_snapshot
from dualmation.experiment.logging_setup import setup_logging
from dualmation.experiment.tracker import ExperimentTracker, TrackerConfig
from dualmation.reward.reward_model import RewardConfig, RewardModel, RewardScore

logger = logging.getLogger(__name__)


@dataclass
class PipelineResult:
    """Result from a single pipeline run.

    Attributes:
        concept: The input concept.
        generated_code: The Manim Python code generated by the LLM.
        background_images: Diffusion-generated background images.
        composited_frames: Final composited output frames.
        reward: Reward score from the RL scorer.
        concept_embedding: The concept's embedding in the shared space.
    """

    concept: str
    generated_code: str = ""
    background_images: list[Image.Image] = field(default_factory=list)
    composited_frames: list[Image.Image] = field(default_factory=list)
    reward: RewardScore | None = None
    concept_embedding: torch.Tensor | None = None


class DualAnimatePipeline:
    """Full DualAnimate inference pipeline.

    Orchestrates the flow:
    1. Encode concept â†’ multimodal embedding
    2. LLM generates Manim code (Brain 1: Logic)
    3. Diffusion generates visual background (Brain 2: Aesthetics)
    4. Alpha compositor merges foreground + background
    5. Reward model scores the output
    6. (Training mode) RL feedback updates LLM and diffusion

    The pipeline reads its configuration from ExperimentConfig (YAML files
    in configs/) and integrates with the experiment tracking framework.

    Args:
        config: ExperimentConfig loaded from YAML or constructed programmatically.
    """

    def __init__(self, config: ExperimentConfig) -> None:
        self.config = config
        self.device = self._resolve_device(config.device)

        # Set seed for reproducibility
        set_seed(config.seed)

        # Build sub-configs from ExperimentConfig
        composite_cfg = CompositeConfig(
            output_width=config.compositor.output_width,
            output_height=config.compositor.output_height,
            blend_mode=config.compositor.blend_mode,
            background_opacity=config.compositor.background_opacity,
            foreground_opacity=config.compositor.foreground_opacity,
        )
        reward_cfg = RewardConfig(
            weight_alignment=config.reward.weight_alignment,
            weight_visual=config.reward.weight_visual,
            weight_compilation=config.reward.weight_compilation,
        )

        # Modules are lazy-loaded to avoid loading everything at init
        self._code_encoder = None
        self._visual_encoder = None
        self._code_generator = None
        self._visual_generator = None
        self._compositor = AlphaCompositor(composite_cfg)
        self._reward_model = RewardModel(reward_cfg, device=self.device)

        # Experiment tracker (optional)
        self._tracker: ExperimentTracker | None = None

    @staticmethod
    def _resolve_device(device: str) -> str:
        """Resolve 'auto' device to actual device string."""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    @classmethod
    def from_config_file(cls, config_path: str | Path, overrides: dict | None = None) -> DualAnimatePipeline:
        """Create a pipeline from a YAML config file.

        Args:
            config_path: Path to YAML config file (e.g., configs/default.yaml).
            overrides: Optional dict of overrides (supports dot notation).

        Returns:
            Configured DualAnimatePipeline instance.
        """
        config = load_config(config_path, overrides=overrides)
        return cls(config)

    def attach_tracker(self, tracker: ExperimentTracker) -> None:
        """Attach an experiment tracker for logging metrics during runs."""
        self._tracker = tracker

    # â”€â”€ Lazy-loaded modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @property
    def code_encoder(self):
        """Lazy-load the code encoder."""
        if self._code_encoder is None:
            from dualmation.embeddings.code_encoder import CodeEncoder

            self._code_encoder = CodeEncoder(
                model_name=self.config.embedding.code_model,
                embedding_dim=self.config.embedding.embedding_dim,
            ).to(self.device)
        return self._code_encoder

    @property
    def visual_encoder(self):
        """Lazy-load the visual encoder."""
        if self._visual_encoder is None:
            from dualmation.embeddings.visual_encoder import VisualEncoder

            self._visual_encoder = VisualEncoder(
                model_name=self.config.embedding.visual_model,
                embedding_dim=self.config.embedding.embedding_dim,
            ).to(self.device)
        return self._visual_encoder

    @property
    def code_generator(self):
        """Lazy-load the LLM code generator."""
        if self._code_generator is None:
            from dualmation.llm.code_generator import ManimCodeGenerator

            self._code_generator = ManimCodeGenerator(
                model_name=self.config.llm.model_name,
                device=self.device,
            )
        return self._code_generator

    @property
    def visual_generator(self):
        """Lazy-load the diffusion visual generator."""
        if self._visual_generator is None:
            from dualmation.diffusion.visual_generator import VisualGenerator

            self._visual_generator = VisualGenerator(
                model_name=self.config.diffusion.model_name,
                device=self.device,
            )
        return self._visual_generator

    # â”€â”€ Pipeline execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self, concept: str | None = None) -> PipelineResult:
        """Execute the full pipeline for a concept.

        Args:
            concept: Override concept from config if provided.

        Returns:
            PipelineResult with all generated artifacts and scores.
        """
        concept = concept or self.config.experiment_name
        result = PipelineResult(concept=concept)

        logger.info("ðŸš€ Pipeline starting for concept: %s", concept)
        logger.info("ðŸ“‹ Config: seed=%d, device=%s", self.config.seed, self.device)

        # Step 1: Generate concept embedding (optional)
        embedding = None
        try:
            embedding = self.code_encoder.encode(concept)
            result.concept_embedding = embedding
            logger.info("âœ… Concept embedding generated: shape=%s", embedding.shape)
            if self._tracker:
                self._tracker.log_scalar("pipeline/embedding_norm", embedding.norm().item())
        except Exception as e:
            logger.warning("âš ï¸ Embedding generation failed, continuing without: %s", e)

        # Step 2: LLM â†’ Manim code (Brain 1: Logic)
        try:
            code = self.code_generator.generate_with_embedding(
                concept=concept, embedding=embedding
            )
            result.generated_code = code
            logger.info("âœ… Manim code generated: %d chars", len(code))
            if self._tracker:
                self._tracker.log_scalar("pipeline/code_length", len(code))
                self._tracker.log_text("pipeline/generated_code", code)
        except Exception as e:
            logger.error("âŒ Code generation failed: %s", e)
            result.generated_code = f"# Code generation failed: {e}"

        # Step 3: Diffusion â†’ visual background (Brain 2: Aesthetics)
        try:
            backgrounds = self.visual_generator.generate_with_embedding(
                concept=concept, embedding=embedding
            )
            result.background_images = backgrounds
            logger.info("âœ… Background generated: %d images", len(backgrounds))
            if self._tracker and backgrounds:
                self._tracker.log_image("pipeline/background", backgrounds[0])
        except Exception as e:
            logger.warning("âš ï¸ Background generation failed: %s", e)

        # Step 4: Compositor (if we have both layers)
        if result.background_images:
            logger.info("âœ… Compositor ready (awaiting Manim render for foreground)")

        # Step 5: Reward scoring
        try:
            visual = result.background_images[0] if result.background_images else None
            reward = self._reward_model.score(
                code=result.generated_code,
                visual=visual,
                concept=concept,
                concept_embedding=embedding,
            )
            result.reward = reward
            logger.info(
                "âœ… Reward: total=%.3f (align=%.3f, visual=%.3f, compile=%.3f)",
                reward.total, reward.concept_alignment,
                reward.visual_quality, reward.compilation_success,
            )
            if self._tracker:
                self._tracker.log_scalars("reward", {
                    "total": reward.total,
                    "alignment": reward.concept_alignment,
                    "visual_quality": reward.visual_quality,
                    "compilation": reward.compilation_success,
                })
        except Exception as e:
            logger.warning("âš ï¸ Reward scoring failed: %s", e)

        # Save outputs
        self._save_outputs(result)

        logger.info("ðŸŽ¬ Pipeline complete for: %s", concept)
        return result

    def _save_outputs(self, result: PipelineResult) -> None:
        """Save all generated artifacts to the output directory."""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save generated code
        code_path = output_dir / "generated_scene.py"
        code_path.write_text(result.generated_code)
        logger.info("Saved code: %s", code_path)

        # Save background images
        for i, img in enumerate(result.background_images):
            img_path = output_dir / f"background_{i:04d}.png"
            img.save(img_path)

        # Save composited frames
        for i, frame in enumerate(result.composited_frames):
            frame_path = output_dir / f"composited_{i:04d}.png"
            frame.convert("RGB").save(frame_path)

        # Save reward summary
        if result.reward:
            reward_path = output_dir / "reward_summary.txt"
            reward_path.write_text(
                f"Concept: {result.concept}\n"
                f"Total Reward: {result.reward.total:.4f}\n"
                f"Concept Alignment: {result.reward.concept_alignment:.4f}\n"
                f"Visual Quality: {result.reward.visual_quality:.4f}\n"
                f"Compilation Success: {result.reward.compilation_success:.4f}\n"
                f"Compilation Output: {result.reward.compilation_output}\n"
            )
